# AI Pipeline â€” SE347_Backend/AI

This folder contains the data collection, preprocessing, and model artifacts for the real-estate price-per-m2 prediction pipeline (Ho Chi Minh City).

## Overview ğŸ”§
- Crawl: collect listing URLs using `GetLinkNhaDat.py` (page-level crawler) and save links to `linkNhaDat.txt`.
- Scrape details: `LocDataLink.py` reads `linkNhaDat.txt`, scrapes each listing and writes JSON lines to `data1.json`.
- Convert: `json_to_csv.py` converts `data1.json` to `data_1.csv` (CSV schema in code).
- Dedupe: `dedup_data.py` removes duplicate rows (exact or fuzzy) and writes `data_1.dedup.csv`.
- Preprocess: the repository includes preprocessing utilities (e.g., `preprocess_data.py`) to clean and prepare features.
- Train: `train_model.py` trains a RandomForest model and saves it as `model.pkl` (the trained artifact is included here).

## Files of interest âœ…
- `linkNhaDat.txt` â€” scraped listing URLs (added)
- `data1.json` â€” JSON lines of scraped listing details
- `data_1.csv` â€” CSV converted from JSON
- `data_1.dedup.csv` â€” deduplicated CSV
- `json_to_csv.py`, `GetLinkNhaDat.py`, `LocDataLink.py`, `dedup_data.py` â€” pipeline scripts
- `train_model.py` â€” training script (saves model as `RF.pkl`/`model.pkl`)
- `model.pkl` â€” trained model (added)

## Quickstart â€” Requirements âš ï¸
- Python 3.8+ (tested)
- pip packages: `pandas`, `scikit-learn`, `joblib`, `beautifulsoup4`, `undetected-chromedriver`, `selenium`, `psutil`, `lxml`.

Install deps:

```bash
python -m pip install -r requirements.txt
# or individually:
python -m pip install pandas scikit-learn joblib beautifulsoup4 undetected-chromedriver selenium psutil lxml
```

Notes:
- `undetected-chromedriver` and `selenium` will control Chrome; ensure Chrome is installed and compatible with the driver.
- If deploying the crawler, use small delays and respect robots.txt / site terms.

## Usage â€” End-to-end (recommended order) â–¶ï¸
1. Crawl listing links (append to `linkNhaDat.txt`):
   ```bash
   python GetLinkNhaDat.py
   ```
   or to concurrently crawl a page range:
   ```python
   # edit Start/End args in the script or use the multithreading function
   ```

2. Scrape listing details to JSON (append to `data1.json`):
   ```bash
   python LocDataLink.py
   ```

3. Convert JSON -> CSV:
   ```bash
   python json_to_csv.py
   ```
   This writes (appends) into `data_1.csv`.

4. Deduplicate (exact or fuzzy):
   ```bash
   python dedup_data.py --input data_1.csv --method exact
   # or
   python dedup_data.py --input data_1.csv --method fuzzy --coord-scale 1000 --price-tol 0.05 --area-tol 0.10
   ```

5. Preprocess (clean / encode / feature engineering):
   - Run `preprocess_data.py` (if present) or your preprocessing pipeline to produce the final training CSV. Ensure final CSV columns match `train_model.py` expectations.

6. Train model:
   ```bash
   python train_model.py
   ```
   This script trains a RandomForest model and saves it as a pickle file (e.g., `RF.pkl` / `model.pkl`).

## Notes & Caveats ğŸ’¡
- `train_model.py` expects a preprocessed CSV; update the file path variable to the actual preprocessed dataset.
- Scripts write to disk in-place and append to JSON/CSV files; back up your data before re-running.
- Large binary (`model.pkl`) is tracked directly in the repo; consider using Git LFS if models grow >100 MB.

## Troubleshooting ğŸ”§
- Selenium/undetected-chromedriver errors: make sure Chrome & chromedriver versions match and that you have a stable network.
- If the dedupe script raises `KeyError` about coordinates, ensure `tá»a Ä‘á»™ x` and `tá»a Ä‘á»™ y` are present and numeric in the CSV.

## Contact / Next steps âœ¨
If you want, I can:
- Add a usage script to run the entire pipeline end-to-end.
- Add small README sections describing the CSV schema and sample commands.
- Add tests or CI to validate that training finishes and a model is saved.

---
*README generated by GitHub Copilot* (concise pipeline summary)
