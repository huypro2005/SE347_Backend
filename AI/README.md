# Pipeline AI â€” SE347_Backend/AI

ThÆ° má»¥c nÃ y chá»©a cÃ¡c bÆ°á»›c thu tháº­p dá»¯ liá»‡u, tiá»n xá»­ lÃ½ vÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n **giÃ¡ theo triá»‡u/m2** cho báº¥t Ä‘á»™ng sáº£n (TP. HCM).

## Tá»•ng quan ğŸ”§
- Crawl: thu tháº­p link tin báº±ng `GetLinkNhaDat.py` (crawl theo trang) vÃ  lÆ°u vÃ o `linkNhaDat.txt`.
- Scrape chi tiáº¿t: `LocDataLink.py` Ä‘á»c `linkNhaDat.txt`, láº¥y chi tiáº¿t tá»«ng tin vÃ  ghi thÃ nh JSON (má»—i dÃ²ng má»™t Ä‘á»‘i tÆ°á»£ng) vÃ o `data1.json`.
- Chuyá»ƒn Ä‘á»•i: `json_to_csv.py` chuyá»ƒn `data1.json` thÃ nh `data_1.csv`.
- Loáº¡i trÃ¹ng: `dedup_data.py` loáº¡i bá» báº£n ghi trÃ¹ng (exact hoáº·c fuzzy) vÃ  ghi `data_1.dedup.csv`.
- Tiá»n xá»­ lÃ½: sá»­ dá»¥ng script tiá»n xá»­ lÃ½ (vÃ­ dá»¥ `preprocess_data.py`) Ä‘á»ƒ lÃ m sáº¡ch vÃ  táº¡o features.
- Huáº¥n luyá»‡n: `train_model.py` huáº¥n luyá»‡n mÃ´ hÃ¬nh RandomForest vÃ  lÆ°u káº¿t quáº£ dÆ°á»›i dáº¡ng `model.pkl`.

## CÃ¡c táº­p tin quan trá»ng âœ…
- `linkNhaDat.txt` â€” cÃ¡c URL Ä‘Ã£ crawl (Ä‘Ã£ thÃªm)
- `data1.json` â€” káº¿t quáº£ scrape (JSONL)
- `data_1.csv` â€” CSV chuyá»ƒn tá»« JSON
- `data_1.dedup.csv` â€” CSV Ä‘Ã£ loáº¡i trÃ¹ng
- `json_to_csv.py`, `GetLinkNhaDat.py`, `LocDataLink.py`, `dedup_data.py` â€” script pipeline
- `train_model.py` â€” script huáº¥n luyá»‡n (lÆ°u model dÆ°á»›i tÃªn `model.pkl`)
- `model.pkl` â€” mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n (Ä‘Ã£ thÃªm/ghi Ä‘Ã¨ theo yÃªu cáº§u)

## YÃªu cáº§u nhanh âš ï¸
- Python 3.8+
- ThÆ° viá»‡n: `pandas`, `scikit-learn`, `joblib`, `beautifulsoup4`, `undetected-chromedriver`, `selenium`, `psutil`, `lxml`

CÃ i Ä‘áº·t:

```bash
python -m pip install -r requirements.txt
# hoáº·c
python -m pip install pandas scikit-learn joblib beautifulsoup4 undetected-chromedriver selenium psutil lxml
```

LÆ°u Ã½:
- `undetected-chromedriver` vÃ  `selenium` Ä‘iá»u khiá»ƒn Chrome; Ä‘áº£m báº£o Chrome tÆ°Æ¡ng thÃ­ch vá»›i driver.
- Khi cháº¡y crawler trÃªn diá»‡n rá»™ng, tuÃ¢n thá»§ robots.txt vÃ  chÃ­nh sÃ¡ch site; thÃªm delay Ä‘á»ƒ trÃ¡nh bá»‹ block.

## HÆ°á»›ng dáº«n sá»­ dá»¥ng (thá»© tá»± Ä‘á» nghá»‹) â–¶ï¸
1. Crawl link (ghi thÃªm vÃ o `linkNhaDat.txt`):
   ```bash
   python GetLinkNhaDat.py
   ```
2. Scrape chi tiáº¿t vÃ o JSON:
   ```bash
   python LocDataLink.py
   ```
3. Chuyá»ƒn JSON -> CSV:
   ```bash
   python json_to_csv.py
   ```
4. Loáº¡i trÃ¹ng:
   ```bash
   python dedup_data.py --input data_1.csv --method exact
   # hoáº·c fuzzy:
   python dedup_data.py --input data_1.csv --method fuzzy --coord-scale 1000 --price-tol 0.05 --area-tol 0.10
   ```
5. Tiá»n xá»­ lÃ½: cháº¡y script tiá»n xá»­ lÃ½ Ä‘á»ƒ táº¡o CSV huáº¥n luyá»‡n phÃ¹ há»£p.
6. Huáº¥n luyá»‡n mÃ´ hÃ¬nh:
   ```bash
   python train_model.py
   ```

## Ghi chÃº & cáº£nh bÃ¡o ğŸ’¡
- `train_model.py` ká»³ vá»ng dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½; kiá»ƒm tra Ä‘Æ°á»ng dáº«n file Ä‘áº§u vÃ o náº¿u gáº·p lá»—i.
- `json_to_csv.py` vÃ  `LocDataLink.py` cÃ³ hÃ nh vi ghi/appending; sao lÆ°u dá»¯ liá»‡u trÃ¡nh trÃ¹ng khi cháº¡y láº¡i.
- `model.pkl` lá»›n (~65MB) Ä‘Ã£ Ä‘Æ°á»£c commit; náº¿u cÃ¡c mÃ´ hÃ¬nh tiáº¿p theo lá»›n hÆ¡n, cÃ¢n nháº¯c dÃ¹ng Git LFS.

## Há»— trá»£ & bÆ°á»›c tiáº¿p theo âœ¨
Náº¿u muá»‘n, tÃ´i cÃ³ thá»ƒ:
- Chuáº©n hÃ³a `train_model.py` (thÃªm CLI, Ä‘Æ°á»ng dáº«n file) vÃ  commit cÃ¡c thay Ä‘á»•i.
- Táº¡o script cháº¡y end-to-end (`run_pipeline.py`) hoáº·c Makefile.
- ThÃªm CI Ä‘á»ƒ kiá»ƒm tra pipeline/huáº¥n luyá»‡n tá»± Ä‘á»™ng.

---
*README (phiÃªn báº£n tiáº¿ng Viá»‡t) táº¡o bá»Ÿi GitHub Copilot*
